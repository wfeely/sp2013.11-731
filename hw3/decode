#!/usr/bin/env python
#Modified by: Weston Feely & Serena Jeblee
import argparse
import sys
import models
import heapq
from collections import namedtuple

parser = argparse.ArgumentParser(description='Phrase based decoder with local (bigram) reordering.')
parser.add_argument('-i', '--input', dest='input', default='data/input', help='File containing sentences to translate (default=data/input)')
parser.add_argument('-t', '--translation-model', dest='tm', default='data/tm', help='File containing translation model (default=data/tm)')
parser.add_argument('-s', '--stack-size', dest='s', default=1, type=int, help='Maximum stack size (default=1)')
parser.add_argument('-n', '--num_sentences', dest='num_sents', default=sys.maxint, type=int, help='Number of sentences to decode (default=no limit)')
parser.add_argument('-l', '--language-model', dest='lm', default='data/lm', help='File containing ARPA-format language model (default=data/lm)')
parser.add_argument('-v', '--verbose', dest='verbose', action='store_true', default=False,  help='Verbose mode (default=off)')
opts = parser.parse_args()

tm = models.TM(opts.tm, sys.maxint)
lm = models.LM(opts.lm)
sys.stderr.write('Decoding %s...\n' % (opts.input,))
input_sents = [tuple(line.strip().split()) for line in open(opts.input).readlines()[:opts.num_sents]]

hypothesis = namedtuple('hypothesis', 'logprob, lm_state, predecessor, phrase')
for f in input_sents:
    # The following code implements a DP monotone decoding
    # algorithm (one that doesn't permute the target phrases).
    # Hence all hypotheses in stacks[i] represent translations of 
    # the first i words of the input sentence.
    # HINT: Generalize this so that stacks[i] contains translations
    # of any i words (remember to keep track of which words those
    # are, and to estimate future costs)
    initial_hypothesis = hypothesis(0.0, lm.begin(), None, None)

    stacks = [{} for _ in f] + [{}]
    stacks[0][lm.begin()] = initial_hypothesis
    for i, stack in enumerate(stacks[:-1]):
        # extend the top s hypotheses in the current stack
        for h in heapq.nlargest(opts.s, stack.itervalues(), key=lambda h: h.logprob): # prune
	    '''
	    #Allow for local reordering of 1
            if i+1 < len(f):
		#Check if current and next Spanish unigram is in TM
		b = tuple([f[i+1]])
		a = tuple([f[i]])
		if (b in tm) and (a in tm):
			#Generate all phrase translations for these two Spanish unigrams
			phrases_iplusone = [phrase for phrase in tm[b]]
			phrases_i = [phrase for phrase in tm[a]]
			combos = []
			#Get Cartesian product of both phrase sets
			for phrase1 in phrases_iplusone:
				for phrase2 in phrases_i:
					my_en = phrase1.english+' '+phrase2.english
					my_lp = phrase1.logprob+phrase2.logprob
					my_phrase = models.phrase(my_en, my_lp)
					combos.append(my_phrase)
			#Loop through combos and make hypothesis for each combined phrase
			for phrase in combos:
		                logprob = h.logprob + phrase.logprob
		                lm_state = h.lm_state
		                for word in phrase.english.split():
		                    (lm_state, word_logprob) = lm.score(lm_state, word)
		                    logprob += word_logprob
		                logprob += lm.end(lm_state) if i+1 == len(f) else 0.0
		                new_hypothesis = hypothesis(logprob, lm_state, h, phrase)
		                if lm_state not in stacks[i+1] or stacks[i+1][lm_state].logprob < logprob: # second case is recombination
		                    stacks[i+1][lm_state] = new_hypothesis
	    '''
	    #Get continguous phrases from i:j for all j
            for j in xrange(i+1,len(f)+1):
                if f[i:j] in tm:
                    for phrase in tm[f[i:j]]:
                        logprob = h.logprob + phrase.logprob
                        lm_state = h.lm_state
                        for word in phrase.english.split():
                            (lm_state, word_logprob) = lm.score(lm_state, word)
                            logprob += word_logprob
                        logprob += lm.end(lm_state) if j == len(f) else 0.0
                        new_hypothesis = hypothesis(logprob, lm_state, h, phrase)
                        if lm_state not in stacks[j] or stacks[j][lm_state].logprob < logprob: # second case is recombination
                            stacks[j][lm_state] = new_hypothesis
			else:
				continue
			#Allow for local reordering
			for k in xrange(j+1,len(f)+1):
				if f[j:k] in tm:					
					for reorder_phrase in tm[f[j:k]]:
						lp1 = h.logprob + reorder_phrase.logprob
						lms1 = tuple(h.lm_state)
						for w1 in reorder_phrase.english.split():
						    (lms1, wlp1) = lm.score(lms1, w1)
						    lp1 += wlp1
						lp1 += lm.end(lms1) if k == len(f) else 0.0
						reorder_parent = hypothesis(lp1, lms1, h, reorder_phrase)
						#if lms1 not in stacks[k] or stacks[k][lms1].logprob < lp1:
						#    stacks[k][lms1] = reorder_parent
						#else:
						#	continue
						lp2 = reorder_parent.logprob + phrase.logprob
						lms2 = tuple(reorder_parent.lm_state)
						for w2 in phrase.english.split():
						    (lms2, wlp2) = lm.score(lms2, w2)
						    lp2 += wlp2
						lp2 += lm.end(lms2) if j == len(f) else 0.0
						new_hypothesis_2 = hypothesis(lp2, lms2, reorder_parent, phrase)
						if lms2 not in stacks[k] or stacks[k][lms2].logprob < lp2:
							stacks[k][lms2] = new_hypothesis_2

    # find best translation by looking at the best scoring hypothesis
    # on the last stack
    winner = max(stacks[-1].itervalues(), key=lambda h: h.logprob)
    def extract_english_recursive(h):
        return '' if h.predecessor is None else '%s%s ' % (extract_english_recursive(h.predecessor), h.phrase.english)
    print extract_english_recursive(winner)

    if opts.verbose:
        def extract_tm_logprob(h):
            return 0.0 if h.predecessor is None else h.phrase.logprob + extract_tm_logprob(h.predecessor)
        tm_logprob = extract_tm_logprob(winner)
        sys.stderr.write('LM = %f, TM = %f, Total = %f\n' % 
            (winner.logprob - tm_logprob, tm_logprob, winner.logprob))
